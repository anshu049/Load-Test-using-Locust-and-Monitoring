# Introduction
This deep dive explores how to optimize scalability using Locust for load testing on Amazon EKS. We'll focus on scaling a Node.js app using Kubernetes' HPA and Cluster Autoscaler based on load generated by Locust workers. This guide aims to provide practical insights for ensuring applications can handle increasing user loads efficiently.

# Create VPC and EKS using Terraform module
- Apply terraform command.
- Add **policy** to worker to worker node and then **apply cluster-autoscaler-autodiscover.yaml**.
- Add **metric server** for HPA to get data.
- We need to create HPA for both locust and app after deploying them.
- `kubectl autoscale deployment <deploy-name> --cpu-percent=50 --min=1 --max=10`

# Install monitoring components on cluster
- `helm repo add prometheus-communityhttps://prometheus-community.github.io/helm-charts`
- `helm repo update`
- `kubectl create ns monitoring`
- `helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring`
- Expose service to access UI of Prometheus and Grafana.

# Deploy Loust
- Update the Configmap with the actual url of app before applying.

# Build Docker image using Github Action(CI)
- Github Action automatically creates new Image from the latest changes made in app.
  
# Deployment of App using ArgoCD(CD)
![argo](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/7f3d931b-a8ec-4543-947d-291708a064d4)

# Demo
- Access UI of locust and Run Test
![adding-url-to-locust](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/7434ca15-435d-4c13-b3d2-26ce97335dbc)

## Grafana Dashboard
- request/second generated on our app by Locust
![request:second-final](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/92c36e31-8c81-4160-ade8-41aaa1af8f2a)

- Total request generated on our app by Locust.
![total-request-generated-final](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/6e081252-9d71-4863-bbe6-ab246e94dc2a)

- Increase in cluster resource as the new pod and nodes are getting created.

![locust-overview](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/d4b11fc4-1cbd-47f5-a961-ef70bea70f58)

Let it run for a few minutes and, in the meantime, switch between the Statistics tab and the Charts tab to see how the test is unfolding.

![locust-logs](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/f802dd6d-5787-474a-b3ac-b81f752fbaed)
We can see 69 locust workers created using HPA to generate load on our app

# Now, Lets see how our cluster scaled
- As load increased on our app, HPA created new pod based on resource utilisation.
![hpa](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/45abf727-5d98-43de-b863-42b2092c5465)

- Available node is not enough for new pod get running, So new pod is in "pending state"
![pods-pending](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/4cda45dd-5697-4880-8c88-7a14510b5dae)

- New node get created automatically.
![new-node-created](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/88a173c8-2e73-4ad2-9248-482279cbb5e6)

- Still the pod is in "pending state", as soon as the new worker node gets ready
![new-node-ready](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/dc117722-afc5-4e49-853e-dd9d64393e2b)

- The pod state is changed to "running"
![new-pod-started](https://github.com/anshu049/Load-Test-using-Locust-and-Monitoring/assets/95365748/3b21cd0d-70ea-45c2-a6a2-422a6efe79c5)


# Conclusion 
In summary, this deep dive into load testing with Locust on Amazon EKS focused on optimizing scalability for a Node.js application using Kubernetes' HPA and Cluster Autoscaler. Key steps included setting up EKS clusters, applying worker node autoscaling policies, enabling HPA, integrating the Cluster Autoscaler, and deploying monitoring components like Prometheus and Grafana. The process demonstrated how the system dynamically scaled resources in response to increased load, ensuring efficient handling of user traffic. Overall, the guide provides practical insights for developers and DevOps teams to enhance scalability and performance in Kubernetes environments.

*Delete all the resources*
